# This script is used to query and analyze the time series data
# generated by 'example_time_series_data_stream.py'.
# Make sure to run that script first to ingest some data.
from elasticsearch import Elasticsearch
import psutil
from datetime import datetime
import matplotlib.pyplot as plt

es = Elasticsearch("http://localhost:9200")
client_info = es.info()


# The alias for the data stream, defined in the index template
index_alias = "cpu_example_template"

# Get the total number of documents currently in the data stream
count = es.count(index=index_alias)
print(f"Total documents in {index_alias}: {count['count']}")

# Retrieve the last 1000 documents from the data stream to visualize
response = es.search(
    index="cpu_example_template",
    body={
        "query": {
            "match_all": {}
        },
        "size": 1000
    },
)
hits = response.body["hits"]["hits"]

# Extract values for plotting
cpu_usage_values = [hit["_source"]["cpu_usage"] for hit in hits]
timestamp_values = [hit["_source"]["@timestamp"] for hit in hits]

# The following lines are for plotting the CPU usage over time.
# Uncomment them if you have matplotlib installed and want to see a graph.
# plt.figure(figsize=(10, 5))
# plt.plot(timestamp_values, cpu_usage_values)
# plt.xticks([])
# plt.xlabel("Timestamp")
# plt.ylabel("CPU Usage (%)")
# plt.title("CPU usage over time")
# plt.grid(True)
# plt.show()

# Use an aggregation to calculate the average CPU usage across all documents
response = es.search(
    index="cpu_example_template",
    body={
        "aggs": {
            "avg_cpu_usage": {
                "avg": {
                    "field": "cpu_usage"
                }
            }
        },
    },
)
average_cpu_usage = response.body["aggregations"]["avg_cpu_usage"]["value"]
print(f"Average CPU usage: {average_cpu_usage}%")

# Use an aggregation to find the maximum CPU usage recorded
response = es.search(
    index="cpu_example_template",
    body={
        "aggs": {
            "max_cpu_usage": {
                "max": {
                    "field": "cpu_usage"
                }
            }
        },
    },
)
max_cpu_usage = response.body["aggregations"]["max_cpu_usage"]["value"]
print(f"Max CPU usage: {max_cpu_usage}%")


# Get detailed information about the data stream, including the backing indices
response = es.indices.get_data_stream(name=index_alias)
print("--- Data Stream Info ---")
print(response.body)

# Retrieve the configuration of the ILM policy
ilm_status = es.ilm.get_lifecycle(name="cpu_usage_policy_v2")
print("--- ILM Policy Info ---")
print(ilm_status.body)

# Check the current lifecycle status of the backing indices
# This shows the current phase (e.g., hot) and time until the next action (e.g., rollover)
response = es.ilm.explain_lifecycle(
    index=f".ds-{index_alias}*")
print("--- ILM Status Explanation ---")
print(response.body)

# IMPORTANT: Data Loss in Delete Phase
# When ILM policy deletes old indices, that data is PERMANENTLY GONE!
# 
# Current policy: Data is deleted after 20 minutes (min_age: "20m")
# This means you only keep the last 20 minutes of CPU data.
#
# Trade-offs to consider:
# ✅ PROS of automatic deletion:
#   - Prevents storage from growing infinitely
#   - Keeps cluster performance optimal
#   - No manual cleanup needed
#
# ❌ CONS of automatic deletion:
#   - Historical data is lost forever
#   - No long-term trend analysis possible
#   - Cannot recover deleted data
#
# ALTERNATIVES if you need to keep historical data:
# 1. Extend the delete phase: "min_age": "30d" (30 days instead of 20 minutes)
# 2. Add warm/cold phases to move data to cheaper storage before deleting
# 3. Use snapshots to backup data before deletion
# 4. Stream data to external systems (data lakes, warehouses) for long-term storage